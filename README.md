In this approach, we fine-tuned a pretrained BERT (bert-base-uncased) model for sentiment classification due to its strong contextual understanding and proven effectiveness in NLP tasks. Minimal text preprocessing was applied to retain syntactic structure, as BERTâ€™s tokenizer is designed to handle punctuation and casing. A custom PyTorch Dataset and DataLoader enabled efficient batching and GPU utilization. We used a stratified train-validation split to ensure label balance during evaluation. The linear learning rate scheduler with AdamW optimizer was chosen to enhance convergence stability. We evaluated model performance using the weighted F1 score, which is appropriate for handling class imbalance. Our predictions assumed similar label distribution and language structure in the test data, and we relied solely on the tweet text, presuming it contained enough sentiment signals. This methodology balances computational efficiency with robust performance in real-world tweet sentiment classification scenarios.
